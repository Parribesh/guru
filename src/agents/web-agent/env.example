# Electron AI Browser - Environment Configuration
# Copy this file to .env and update the values as needed
# 
# Usage:
#   cp env.example .env
#   # Then edit .env with your actual values

# =============================================================================
# Port Configuration
# =============================================================================

# Webpack dev server port (for renderer process hot reload)
# Default: 3000
WEBPACK_DEV_SERVER_PORT=3000

# CLI server port (for command-line interface)
# Default: 9876
CLI_SERVER_PORT=9876

# =============================================================================
# Embedding Service Configuration
# =============================================================================

# Base URL for the Python embedding service
# Default: http://127.0.0.1:8000
EMBEDDING_SERVICE_URL=http://127.0.0.1:8000

# WebSocket URL for real-time progress monitoring
# Default: ws://127.0.0.1:8000/ws (auto-generated from EMBEDDING_SERVICE_URL if not set)
# EMBEDDING_SERVICE_SOCKET_URL=ws://127.0.0.1:8000/ws

# Optional API key for embedding service authentication
# EMBEDDING_SERVICE_API_KEY=your-api-key-here

# Batch size for embedding generation (number of chunks per batch)
# Should match or be compatible with worker_batch_size from the Python service
# Default: 4
EMBEDDING_BATCH_SIZE=4

# Timeout for embedding task completion (in milliseconds)
# Default: 30000 (30 seconds)
EMBEDDING_TIMEOUT=30000

# =============================================================================
# Application Configuration
# =============================================================================

# Node environment (development, production, test)
# Default: development
NODE_ENV=development

# Enable/disable webpack hot module replacement
# Default: true (set to false to disable)
# WEBPACK_HOT=true

# =============================================================================
# AI/LLM Service Configuration
# =============================================================================

# If you're using external AI services, you can configure them here
# OPENAI_API_KEY=your-openai-key
# ANTHROPIC_API_KEY=your-anthropic-key

# Ollama configuration (if using local Ollama for LLM)
# Default: http://localhost:11434
OLLAMA_URL=http://localhost:11434

# AI model to use (for Ollama or other local models)
# Default: llama3.2:1b
AI_MODEL=llama3.2:1b

# =============================================================================
# Optional: Transformers Cache Configuration
# =============================================================================

# Custom cache directory for @xenova/transformers models
# Default: ~/.cache/huggingface/hub
# TRANSFORMERS_CACHE=/path/to/custom/cache
